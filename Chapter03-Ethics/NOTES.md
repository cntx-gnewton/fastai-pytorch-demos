
# Chapter 03: Ethics

## Homework Assignment 3

### Questionnaire

1. Does ethics provide a list of "right answers"?
2. How can working with people of different backgrounds help when considering ethical questions?
   - It can help to avoid groupthink and to consider a wider range of perspectives.
3. What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?
   - provided data tabulation products necessary to track the extermination of Jews and other groups on a massive scale.
   - the means were more important than the ends.
   - The workers participated because their job is apart of their everyday lives and they were doing their jobs in order to not get fired.

4. What was the role of the first person jailed in the Volkswagen diesel scandal?
   - The first person jailed in the Volkswagen diesel scandal was an engineer who helped design the software that enabled the cars to cheat on emissions tests.
5. What was the problem with a database of suspected gang members maintained by California law enforcement officials?
    - The database was found to be riddled with errors and false positives.
6. Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even though no employee at Google had programmed this feature?
    - The recommendation algorithm was designed to maximize watch time.
7. What are the problems with the centrality of metrics?
    - The problem with the centrality of metrics is that it can lead to a focus on what is measured, rather than what is important.
8. Why did Meetup.com not include gender in its recommendation system for tech meetups?
    - So that tech meetups would be recommended with equal probability independent of gender.
9. What are the six types of bias in machine learning, according to Suresh and Guttag?
     - Historical bias: people are biased, processes are biased, and society is biased.
     - Measurement bias: occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.
     - Aggregation bias: occurs when we aggregate data in a way that hides important information.
     - Representation bias: occurs when the data we have doesn't represent the true distribution of the population.
     - Evaluation bias: occurs when our models make mistakes because we are evaluating them in the wrong way.
     - Deployment bias: occurs when our models make mistakes because they are deployed in the wrong way.
10. Give two examples of historical race bias in the US.
    - When doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients.
    - When bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions.
11. Where are most images in ImageNet from?
    - United States and other Western countries
12. In the paper ["Does Machine Learning Automate Moral Hazard and Error"](https://scholar.harvard.edu/files/sendhil/files/aer.p20171084.pdf) why is sinusitis found to be predictive of a stroke?
    - Because people with sinusitis are more likely to see a doctor, and therefore more likely to be diagnosed with a stroke.
13. What is representation bias?
    - Representation bias occurs when the data we have doesn't represent the true distribution of the population.
14. How are machines and people different, in terms of their use for making decisions?
    - Machines have an ultra defined scope of knowledge that is used to make action policies, humans have a much broader scope of knowledge and can make decisions based on a much broader range of information.
15. Is disinformation the same as "fake news"?
    - No, disinformation is the deliberate creation and sharing of information that doesn't represent the truth, while fake news is a subset of disinformation.
16. Why is disinformation through auto-generated text a particularly significant issue?
    - Auto-generated text can be used to create large amounts of disinformation that is difficult to trace back to its source.
17. What are the five ethical lenses described by the Markkula Center?
    - The rights approach:: Which option best respects the rights of all who have a stake?
    - The justice approach:: Which option treats people equally or proportionately?
    - The utilitarian approach:: Which option will produce the most good and do the least harm?
    - The common good approach:: Which option best serves the community as a whole, not just some members?
    - The virtue approach:: Which option leads me to act as the sort of person I want to be?
18. Where is policy an appropriate tool for addressing data ethics issues?
    - Policy is an appropriate tool for addressing data ethics issues when the issue is systemic and motivates markets to develop products/services that are to the detriment of society.
  
### Further Research

1. Read the article "What Happens When an Algorithm Cuts Your Healthcare". How could problems like this be avoided in the future?
  - Problems like this could be avoided in the future by implementing recourse processes that allow people to challenge algorithms and the decisions made by them.
2. Research to find out more about YouTube's recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take to avoid them? What about the government?
 -  Feedback loops are necessary to train recommendation systems. Even though it might seem like negative feedback loops are unavoidable, Google could implement a system that allows users to flag videos that are inappropriate and then use that data to train the recommendation system to avoid recommending similar videos in the future. The government could also implement regulations that require companies to implement such systems.
3. Read the paper ["Discrimination in Online Ad Delivery"](https://arxiv.org/abs/1301.6822). Do you think **Google** should be considered responsible for what happened to Dr. Sweeney? What would be an appropriate response?
    - Yes, Google should be considered responsible for what happened to Dr. Sweeney. It is their ethical responsibility to take accountability for the effects of their products to society. An appropriate response would be to implement a system that allows users to flag ads that are inappropriate and then use that data to train the ad delivery system to avoid delivering similar ads in the future.
4. How can a cross-disciplinary team help avoid negative consequences?
    - A cross-disciplinary team can help broaden the perspectives and experiences that are considered when making product development decisions, which can help avoid negative consequences through mitigating biases.
5. Read the paper "Does Machine Learning Automate Moral Hazard and Error". What actions do you think should be taken to deal with the issues identified in this paper?
 - Improve data quality
 - Regularly audit and evaluate the performance of machine learning models.
 - Use explainable AI techniques in order for results to be interpretable.
6. Read the article "How Will We Prevent AI-Based Forgery?" Do you think Etzioni's proposed approach could work? Why?
 - Technically, I think it could work. Realistically, I think it would be difficult to implement and enforce. It would require a lot of cooperation between different organizations and governments, and the general public would have to be educated on how to verify the authenticity of digital content and the importance of doing so in order for systematic adoption to take place.
8. Complete the section "Analyze a Project You Are Working On" in this chapter.
  - Should we even be doing this?
    - Yes, the project is to develop an overarching data model, data governance, and data science models for the operations of a mid-stream oil and gas company.
  - What bias is in the data?
    - Measurement bias in the meters that are used to measure the flow of oil and gas.
    - Aggregation bias in the way that the data is aggregated and stored.
  - Can the code and data be audited?
   - Yes, the code undergoes strict CI/CD processes with code reviews.
  - What are the error rates for different sub-groups?
    - The error rates for different sub-groups are unknown.
  - What is the accuracy of a simple rule-based alternative?
    - Very low.
  - What processes are in place to handle appeals or mistakes?
    - There is a dashboard that allows users to see the data and make decisions based on the data. If they have an issue with the ML model, they can appeal to the data science team.
  - How diverse is the team that built it?
    - Extremly diverse.
9.  Consider whether your team could be more diverse. If so, what approaches might help?
  - Maybe having a software engineer and a CI/CD engineer on the team would help.  Cross-training in other areas. But we have a diverse team already.


## Notes

- Ethics

  - Well-founded standards of right and wrong that prescribe what humans ought to do
  - The study and development of one's ethical standards.

- Data Ethics

  - Subfield of Ethics
  - Examples
    1. *Recourse processes*—Arkansas's buggy healthcare algorithms left patients stranded.
       - Auditting processes that allow people to challenge algoirithms and the decisions made by them.
    2. *Feedback loops*—YouTube's recommendation system helped unleash a conspiracy theory boom.
    3. *Bias*—When a traditionally African-American name is searched for on Google, it displays ads for criminal background checks.
       - Historical bias: people are biased, processes are biased, and society is biased.
       - Measurement bias: occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.
       - Aggregation bias: occurs when we aggregate data in a way that hides important information.
       - Representation bias: occurs when the data we have doesn't represent the true distribution of the population.
    4. *Disinformation*—Auto-generated text is a significant issue.

- Why does Data Ethics Matter?

  - A company's data ethics policies define the bottom line of what is acceptable and what is not.
  - Having strong data ethics policies can guide a company to making business decisions that are not only profitable but also for the betterment of society.

- Addressing Data Ethics

  1. *Bias* - All datasets contain bias.  There is no such thing as a completely debiased dataset.
     - Historical bias: people are biased, processes are biased, and society is biased.
     - Measurement bias: occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.
     - Aggregation bias: occurs when we aggregate data in a way that hides important information.
     - Representation bias: occurs when the data we have doesn't represent the true distribution of the population.
  2. *Disinformation* - The deliberate creation and sharing of information that doesnt represent the truth.
     - develop some form of digital signature
  3. General Steps:
     - Analyze a project you are working on.
     - Implement processes at your company to find and address ethical risks.
     - Support good policy.
     - Increase diversity.

- Data Ethics Frameworks: Each project should be analyzed to determine the best ethical framework to use. Use different frameworks to evaluate the **Fairness**, **Accountability**, and **Transparency** of the final proposed ML Product being developed.
  - Markkula Center for Applied Ethics
    - Utilitarianism
    - Rights
    - Fairness or Justice
    - Common Good
    - Virtue
    - Care
  - Deontological
    - What rights of others and duties to others must we respect?
    - How might the dignity and autonomy of each stakeholder be impacted by this project?
    - What considerations of trust and of justice are relevant to this design/project?
    - Does this project involve any conflicting moral duties to others, or conflicting stakeholder rights? How can we prioritize these?

- ML Product Design
  - Should you focus on simplicity of implementation, speed of inference, or accuracy of the model?
  - How will your model handle out-of-domain data items?
  - Can it be fine-tuned, or must it be retrained from scratch over time?
  - Whose interests, desires, skills, experiences, and values have we simply assumed, rather than actually consulted?
  - Who are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are—have we asked?
  - Who/which groups and individuals will be indirectly affected in significant ways?
  - Who might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend?
